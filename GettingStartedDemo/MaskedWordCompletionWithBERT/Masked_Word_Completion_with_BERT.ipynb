{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masked Word Completion with BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hvh9hYvaPkvU",
        "s-nfX4WQ_wWF",
        "vJXc4OWhPopw",
        "1mQyHwlz_CpX",
        "7zPxWsOqD6OR",
        "DdrfeCqVHk4Q"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvh9hYvaPkvU"
      },
      "source": [
        "# 1. 环境准备"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-nfX4WQ_wWF"
      },
      "source": [
        "## 1.1 安装transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJyNxtF__zYW"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJXc4OWhPopw"
      },
      "source": [
        "## 1.2 查看GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg74WaPgPr1_",
        "outputId": "4237a6ab-f9c1-4832-ec4d-3877a99e1a0b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug  3 09:01:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    35W / 250W |   2463MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znWEUUnT-_Vq"
      },
      "source": [
        "# 4. How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mQyHwlz_CpX"
      },
      "source": [
        "## 4.1 可以通过pipeline来直接使用这个模型进行掩码语言建模问题（MLM）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RKQ3HieBnzG"
      },
      "source": [
        "作者提供的英文demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVNEOZCI_CLe",
        "outputId": "3d7ca839-262c-48e6-b3f5-aeebea755e4d"
      },
      "source": [
        "from transformers import pipeline # transformers是基准，pipeline理解为一种执行器？\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased') # 任务：fill-mask，模型：bert-base-uncased\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.10731059312820435,\n",
              "  'sequence': \"hello i'm a fashion model.\",\n",
              "  'token': 4827,\n",
              "  'token_str': 'fashion'},\n",
              " {'score': 0.08774515986442566,\n",
              "  'sequence': \"hello i'm a role model.\",\n",
              "  'token': 2535,\n",
              "  'token_str': 'role'},\n",
              " {'score': 0.05338393896818161,\n",
              "  'sequence': \"hello i'm a new model.\",\n",
              "  'token': 2047,\n",
              "  'token_str': 'new'},\n",
              " {'score': 0.04667220264673233,\n",
              "  'sequence': \"hello i'm a super model.\",\n",
              "  'token': 3565,\n",
              "  'token_str': 'super'},\n",
              " {'score': 0.027095947414636612,\n",
              "  'sequence': \"hello i'm a fine model.\",\n",
              "  'token': 2986,\n",
              "  'token_str': 'fine'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvmMEk5aBh0T"
      },
      "source": [
        "自己做一个测试，看看bert-chinese能不能用于中文填充"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU0WN1qu-08C",
        "outputId": "375ea7ad-f5ca-4b2e-a9d3-dd27890de545"
      },
      "source": [
        "from transformers import pipeline # transformers是基准，pipeline理解为一种执行器？\n",
        "unmasker = pipeline('fill-mask', model='bert-base-chinese') # 任务：fill-mask，模型：bert-base-uncased\n",
        "a = unmasker(\"北京是[MASK]国的首都.\")\n",
        "print(a[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'sequence': '北 京 是 中 国 的 首 都.', 'score': 0.9919257760047913, 'token': 704, 'token_str': '中'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOwnRCSgPc5g",
        "outputId": "2c87febf-81e3-4af8-8d92-77317315762d"
      },
      "source": [
        "from transformers import pipeline # transformers是基准，pipeline理解为一种执行器？\n",
        "unmasker = pipeline('fill-mask', model='bert-base-chinese') # 任务：fill-mask，模型：bert-base-uncased\n",
        "unmasker(\"张伯伯，李伯伯，饽饽[MASK]里买饽饽.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3059212267398834,\n",
              "  'sequence': '张 伯 伯 ， 李 伯 伯 ， 饽 饽 哪 里 买 饽 饽.',\n",
              "  'token': 1525,\n",
              "  'token_str': '哪'},\n",
              " {'score': 0.0894886776804924,\n",
              "  'sequence': '张 伯 伯 ， 李 伯 伯 ， 饽 饽 这 里 买 饽 饽.',\n",
              "  'token': 6821,\n",
              "  'token_str': '这'},\n",
              " {'score': 0.08113468438386917,\n",
              "  'sequence': '张 伯 伯 ， 李 伯 伯 ， 饽 饽 家 里 买 饽 饽.',\n",
              "  'token': 2157,\n",
              "  'token_str': '家'},\n",
              " {'score': 0.06640081107616425,\n",
              "  'sequence': '张 伯 伯 ， 李 伯 伯 ， 饽 饽 店 里 买 饽 饽.',\n",
              "  'token': 2421,\n",
              "  'token_str': '店'},\n",
              " {'score': 0.05617409572005272,\n",
              "  'sequence': '张 伯 伯 ， 李 伯 伯 ， 饽 饽 那 里 买 饽 饽.',\n",
              "  'token': 6929,\n",
              "  'token_str': '那'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zPxWsOqD6OR"
      },
      "source": [
        "## 4.2 这里介绍了如何使用此模型来获取Pytorch中给定文本的特征"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ4S3qcxELSR",
        "outputId": "8638cadc-c005-493d-d475-09d0b04b1a35"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "text = \"Germany beat Argentina 2-0 in the World Cup Final.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "print(output['pooler_output'].shape) # \n",
        "print(output['last_hidden_state'].shape) # torch.Size([1, 14, 768])\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.7806, -0.5052, -0.4803,  ..., -0.5686,  0.5664,  0.0422],\n",
            "         [-0.2767, -0.0094, -0.5687,  ..., -0.9396,  0.5665,  0.5992],\n",
            "         [-0.6992, -0.5184,  0.0645,  ..., -0.5637,  0.3959, -0.1853],\n",
            "         ...,\n",
            "         [-0.0658, -0.3086,  0.4791,  ..., -0.7885,  0.2312, -0.7588],\n",
            "         [ 0.5805, -0.1045, -0.4543,  ...,  0.0747, -0.4612, -0.3782],\n",
            "         [ 0.7057,  0.0034, -0.4283,  ..., -0.0122, -0.6359, -0.3025]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.9522, -0.5991, -0.9351,  0.8507,  0.8070, -0.3187,  0.9368,  0.4064,\n",
            "         -0.7434, -1.0000, -0.6296,  0.9396,  0.9925,  0.1530,  0.9511, -0.8412,\n",
            "         -0.5568, -0.6162,  0.4094, -0.5936,  0.7155,  1.0000, -0.2538,  0.5125,\n",
            "          0.4883,  0.9832, -0.7997,  0.9457,  0.9806,  0.8179, -0.5707,  0.3542,\n",
            "         -0.9948, -0.1905, -0.9490, -0.9971,  0.5461, -0.8030,  0.0542, -0.2977,\n",
            "         -0.9341,  0.5388,  1.0000, -0.1979,  0.5592, -0.4541, -1.0000,  0.3809,\n",
            "         -0.9398,  0.9456,  0.8527,  0.9807,  0.3103,  0.6939,  0.6657, -0.7179,\n",
            "          0.1432,  0.2381, -0.3910, -0.6857, -0.6786,  0.4696, -0.8718, -0.9645,\n",
            "          0.9291,  0.5706, -0.3385, -0.4458, -0.2223,  0.0719,  0.9240,  0.3917,\n",
            "         -0.4642, -0.7886,  0.6835,  0.4380, -0.6956,  1.0000, -0.5801, -0.9911,\n",
            "          0.8257,  0.7135,  0.6910, -0.1322, -0.1070, -1.0000,  0.5974, -0.3501,\n",
            "         -0.9939,  0.3913,  0.8755, -0.4201,  0.0708,  0.6637, -0.2128, -0.6577,\n",
            "         -0.4684, -0.9384, -0.3707, -0.5255,  0.2231, -0.3300, -0.4999, -0.5034,\n",
            "          0.5616, -0.6614, -0.5587,  0.6305,  0.0787,  0.7401,  0.5535, -0.3815,\n",
            "          0.5971, -0.9836,  0.7479, -0.4561, -0.9950, -0.7400, -0.9967,  0.8303,\n",
            "         -0.5557, -0.4364,  0.9713, -0.1535,  0.5149, -0.2105, -0.9427, -1.0000,\n",
            "         -0.7131, -0.6387, -0.2449, -0.4235, -0.9920, -0.9867,  0.7112,  0.9670,\n",
            "          0.4401,  1.0000, -0.4243,  0.9746, -0.3818, -0.6410,  0.6995, -0.5518,\n",
            "          0.8185,  0.2783, -0.6978,  0.5119, -0.7319,  0.6527, -0.9314, -0.5752,\n",
            "         -0.7300, -0.9312, -0.3233,  0.9692, -0.5383, -0.9545,  0.1631, -0.5557,\n",
            "         -0.5083,  0.8751,  0.7890,  0.4155, -0.2285,  0.5106, -0.3181,  0.6586,\n",
            "         -0.9012, -0.4668,  0.6136, -0.5187, -0.8364, -0.9911, -0.5756,  0.7040,\n",
            "          0.9924,  0.9003,  0.4685,  0.8802, -0.3594,  0.7819, -0.9858,  0.9930,\n",
            "         -0.2991,  0.3518, -0.6968,  0.5042, -0.9218, -0.2326,  0.9266, -0.7199,\n",
            "         -0.8504, -0.2975, -0.4891, -0.5000, -0.8956,  0.7793, -0.3721, -0.4708,\n",
            "         -0.2193,  0.9218,  0.9886,  0.8603,  0.2933,  0.6515, -0.9469, -0.3144,\n",
            "          0.2650,  0.3449,  0.2251,  0.9973, -0.7564, -0.4857, -0.9288, -0.9924,\n",
            "          0.1400, -0.9366, -0.4793, -0.7733,  0.7997, -0.6642,  0.3150,  0.2229,\n",
            "         -0.9725, -0.7440,  0.4992, -0.6974,  0.5771, -0.4633,  0.6717,  0.9676,\n",
            "         -0.7005,  0.6093,  0.9482, -0.9432, -0.8594,  0.6929, -0.4101,  0.9148,\n",
            "         -0.7450,  0.9987,  0.9543,  0.6967, -0.9810, -0.7616, -0.9330, -0.5500,\n",
            "         -0.2956, -0.4872,  0.8714,  0.7407,  0.4614,  0.6320, -0.7407,  0.9979,\n",
            "         -0.6956, -0.9814, -0.9166, -0.2674, -0.9938,  0.9343,  0.3969,  0.7546,\n",
            "         -0.6866, -0.8406, -0.9832,  0.8936,  0.2580,  0.9923, -0.4332, -0.9198,\n",
            "         -0.7724, -0.9790,  0.2011, -0.4179, -0.2469,  0.1904, -0.9741,  0.5852,\n",
            "          0.7408,  0.6842, -0.8570,  0.9995,  1.0000,  0.9891,  0.9336,  0.9245,\n",
            "         -1.0000, -0.8686,  1.0000, -0.9907, -1.0000, -0.9529, -0.7608,  0.4724,\n",
            "         -1.0000, -0.2903, -0.2349, -0.9551,  0.7437,  0.9922,  0.9935, -1.0000,\n",
            "          0.9523,  0.9675, -0.7502,  0.9518, -0.4889,  0.9895,  0.5030,  0.6217,\n",
            "         -0.3970,  0.5054, -0.9448, -0.9002, -0.6425, -0.8224,  0.9999,  0.4098,\n",
            "         -0.8631, -0.9303,  0.8692, -0.1955,  0.0434, -0.9822, -0.2959,  0.8993,\n",
            "          0.7330,  0.3116,  0.3997, -0.7620,  0.4628, -0.2942,  0.3144,  0.7532,\n",
            "         -0.9443, -0.6835, -0.3322,  0.3634, -0.4304, -0.9813,  0.9671, -0.6341,\n",
            "          0.9610,  1.0000,  0.8935, -0.9046,  0.7874,  0.3011, -0.2997,  1.0000,\n",
            "          0.8332, -0.9916, -0.7238,  0.7583, -0.6960, -0.7042,  0.9999, -0.4076,\n",
            "         -0.8144, -0.6586,  0.9940, -0.9962,  0.9990, -0.9135, -0.9878,  0.9802,\n",
            "          0.9504, -0.6706, -0.8529,  0.3481, -0.3459,  0.4223, -0.9657,  0.8260,\n",
            "          0.7112, -0.2993,  0.8544, -0.9153, -0.6950,  0.3973, -0.9180, -0.5613,\n",
            "          0.9731,  0.6346, -0.3378,  0.1987, -0.3681, -0.7126, -0.9778,  0.7489,\n",
            "          1.0000, -0.5645,  0.9458, -0.2886, -0.0543,  0.0113,  0.6877,  0.7292,\n",
            "         -0.4960, -0.8507,  0.9253, -0.9387, -0.9959,  0.8245,  0.3496, -0.2492,\n",
            "          1.0000,  0.5076,  0.4453,  0.5338,  0.9926,  0.1152,  0.4320,  0.8625,\n",
            "          0.9927, -0.4161,  0.7347,  0.9111, -0.9052, -0.3938, -0.7421,  0.0287,\n",
            "         -0.9629,  0.0192, -0.9804,  0.9851,  0.9825,  0.5029,  0.3497,  0.8975,\n",
            "          1.0000, -0.9416,  0.5464, -0.1949,  0.7350, -1.0000, -0.8288, -0.5092,\n",
            "         -0.3060, -0.7879, -0.4193,  0.3304, -0.9904,  0.8107,  0.8615, -0.9608,\n",
            "         -0.9964, -0.5185,  0.8892,  0.4585, -0.9976, -0.7447, -0.7460,  0.6842,\n",
            "         -0.3655, -0.9608, -0.2405, -0.3801,  0.6337, -0.4430,  0.7043,  0.7691,\n",
            "          0.9124, -0.9424, -0.6531, -0.3583, -0.9397,  0.8164, -0.8578, -0.9709,\n",
            "         -0.3334,  1.0000, -0.6023,  0.8274,  0.8066,  0.8054, -0.4524,  0.4244,\n",
            "          0.9264,  0.4173, -0.4144, -0.9493, -0.5341, -0.5744,  0.7595,  0.8745,\n",
            "          0.2641,  0.9268,  0.9109,  0.3810, -0.2273,  0.0775,  0.9998, -0.4105,\n",
            "         -0.1740, -0.5571, -0.1924, -0.4407, -0.0775,  1.0000,  0.2588,  0.7953,\n",
            "         -0.9962, -0.9257, -0.9708,  1.0000,  0.9225, -0.9399,  0.7967,  0.7938,\n",
            "         -0.3423,  0.8023, -0.3644, -0.2848,  0.4239,  0.1400,  0.9890, -0.6301,\n",
            "         -0.9881, -0.8102,  0.6494, -0.9788,  1.0000, -0.7424, -0.4501, -0.6334,\n",
            "         -0.6392,  0.1415,  0.1619, -0.9863, -0.5813,  0.3582,  0.9811,  0.4325,\n",
            "         -0.7625, -0.9504,  0.9559,  0.8631, -0.9564, -0.9639,  0.9889, -0.9819,\n",
            "          0.5940,  1.0000,  0.5385,  0.4820,  0.4560, -0.5717,  0.4627, -0.7894,\n",
            "          0.8399, -0.9477, -0.3140, -0.3892,  0.5799, -0.4343, -0.8909,  0.8785,\n",
            "          0.2974, -0.6593, -0.7048, -0.3218,  0.5932,  0.9363, -0.2085, -0.4667,\n",
            "          0.2286, -0.2161, -0.9339, -0.5911, -0.5306, -1.0000,  0.7451, -1.0000,\n",
            "          0.8105, -0.2204, -0.3393,  0.8756,  0.8230,  0.8375, -0.8851, -0.8582,\n",
            "          0.4618,  0.7464, -0.5154, -0.7844, -0.8406,  0.4118, -0.1430,  0.3062,\n",
            "         -0.6893,  0.7963, -0.4867,  1.0000,  0.3259, -0.7581, -0.9625,  0.4563,\n",
            "         -0.5054,  1.0000, -0.9372, -0.9884,  0.3202, -0.8267, -0.8524,  0.5145,\n",
            "          0.2324, -0.8691, -0.9711,  0.9187,  0.8834, -0.7070,  0.6229, -0.3962,\n",
            "         -0.6836,  0.1643,  0.9506,  0.9936,  0.7899,  0.9361, -0.6021, -0.4306,\n",
            "          0.9784,  0.4516,  0.6330,  0.3424,  1.0000,  0.5136, -0.9380, -0.5090,\n",
            "         -0.9790, -0.2639, -0.9355,  0.4113,  0.4433,  0.9476, -0.4716,  0.9829,\n",
            "         -0.9121,  0.0599, -0.7609, -0.4026,  0.4556, -0.9743, -0.9943, -0.9955,\n",
            "          0.8036, -0.6264, -0.1790,  0.3550,  0.3682,  0.5845,  0.6475, -1.0000,\n",
            "          0.9620,  0.5470,  0.7746,  0.9852,  0.6995,  0.8090,  0.4896, -0.9941,\n",
            "         -0.9656, -0.4715, -0.4918,  0.7842,  0.7700,  0.9003,  0.5846, -0.5466,\n",
            "         -0.6838, -0.6609, -0.8848, -0.9955,  0.4930, -0.7488, -0.9530,  0.9778,\n",
            "         -0.3736, -0.1553,  0.2891, -0.9332,  0.9732,  0.9225,  0.3826,  0.1638,\n",
            "          0.6217,  0.9449,  0.9664,  0.9956, -0.8140,  0.9137, -0.7705,  0.6219,\n",
            "          0.9502, -0.9617,  0.4664,  0.6388, -0.2842,  0.4285, -0.3272, -0.9575,\n",
            "          0.9309, -0.3162,  0.5781, -0.3513, -0.1296, -0.6013, -0.3820, -0.8521,\n",
            "         -0.6866,  0.6828,  0.3076,  0.9499,  0.9138, -0.1880, -0.8080, -0.4152,\n",
            "         -0.5298, -0.9640,  0.9359, -0.1764,  0.2650,  0.7505,  0.0568,  0.9615,\n",
            "          0.1703, -0.5093, -0.6086, -0.8619,  0.9287, -0.8509, -0.6545, -0.7586,\n",
            "          0.8290,  0.4172,  1.0000, -0.7296, -0.9009, -0.6735, -0.6338,  0.3527,\n",
            "         -0.7620, -1.0000,  0.3892, -0.7488,  0.7135, -0.8771,  0.8412, -0.8195,\n",
            "         -0.9812, -0.2492,  0.7214,  0.8044, -0.6100, -0.8924,  0.7204, -0.8367,\n",
            "          0.9816,  0.9163, -0.8569,  0.5833,  0.7696, -0.9301, -0.7031,  0.8943]],\n",
            "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdrfeCqVHk4Q"
      },
      "source": [
        "## 4.3 这里介绍了如何使用此模型来获取TensorFlow中给定文本的特征"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7LZjA2BHn_i",
        "outputId": "4adb31da-f9f4-4bb6-9b3f-21af74bf2a2d"
      },
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}