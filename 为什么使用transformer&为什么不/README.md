# 1. 为什么要用transformers

1. 便于使用先进的模型
   * 在自然语言理解（Natural Language Understanding，NLU）和自然语言生成（Natural Language Generation, NLG）任务上表现优越。自：而实际上自然语言基本就被划分为这两个大类的任务？
   * 对教学和实践友好，且低门槛
   * 高级抽象，只需要了解三个类 -> 指的是哪几个类还需要理解
   * 对所有模型统一的API

2. 更低的计算开销
    * 研究人员可以分享亿级别训练的模型，而非次次从头开始训练
    * 工程师可以减少计算用时和生产环境开销
    * 数十种模型架构、两千多个预训练模型、100多种语言支持

3. 对于模型生命周期的每一个部分都面面俱到：
    * 训练先进的模型只需要3行代码
    * 模型在不同深度学习框架间任意转移
    * 为训练、评估和生产选择最合适的框架，衔接无缝

4. 为需求轻松定制专属模型和用例
    * 作者为每种模型架构提供了多个用例来复现原论文结果
    * 模型内部结构保持透明一致
    * 模型文件可单独使用，方便魔改和快速实验

# 2. 什么情况下不该用 transformers？

* 本库并不是模块化的神经网络工具箱。模型文件中的代码特意呈若璞玉，未经额外抽象封装，以便研究人员快速迭代魔改而不致溺于抽象和文件跳转之中。
  
* Trainer API 并非兼容任何模型，只为本库之模型优化。若是在寻找适用于通用机器学习的训练循环实现，请另觅他库。

* 尽管我们已尽力而为，examples 目录中的脚本也仅为用例而已。对于你的特定问题，它们并不一定开箱即用，可能需要改几行代码以适之。