{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-2_Behind the pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "26DEdVjaHdQ7",
        "ig74G3nMHYth",
        "Ga2CWYaqKltJ",
        "Cs_3wutsNhdB",
        "fpCe8eM7SvKs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26DEdVjaHdQ7"
      },
      "source": [
        "# pip安装transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVkH3DNoHgdV",
        "outputId": "3b110430-ec5e-43f5-c283-52aa83cabbd4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBFyx5D0HPmy"
      },
      "source": [
        "# 2.2.1 视频学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig74G3nMHYth"
      },
      "source": [
        "## stage1，Tokenizer过程的理解"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XVDEpsDHFua",
        "outputId": "51129ed3-f307-4af0-a4fa-c133ca0e4ef0"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # get from https://huggingface.co/models\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga2CWYaqKltJ"
      },
      "source": [
        "## stage2，模型化的理解"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde1p3DVKp4_",
        "outputId": "588a961f-a14b-4263-e116-d1ffd3d88306"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # get from https://huggingface.co/models\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\n",
        "print(outputs)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16, 768])\n",
            "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
            "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
            "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
            "         ...,\n",
            "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
            "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
            "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
            "\n",
            "        [[-0.3088,  0.7332, -0.1861,  ..., -0.1305, -0.9360, -0.0433],\n",
            "         [-0.3340,  0.9830, -0.0946,  ..., -0.3825, -0.6176,  0.2008],\n",
            "         [-0.1687,  0.8781, -0.1117,  ..., -0.2380, -0.7790,  0.0935],\n",
            "         ...,\n",
            "         [-0.3047,  0.7850, -0.2043,  ..., -0.1101, -0.7665, -0.0563],\n",
            "         [-0.4014,  0.8999, -0.2245,  ..., -0.1877, -0.6910,  0.0329],\n",
            "         [-0.2933,  0.7947, -0.2140,  ..., -0.1002, -0.7508, -0.0623]]],\n",
            "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7COUgnUMMno"
      },
      "source": [
        "每个AutoModelForXxx的类加载一个适应于指定任务的模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCdAgi4rMYFU",
        "outputId": "17692fa9-ce30-4343-b410-cf165fe24475"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # get from https://huggingface.co/models\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "\n",
        "print(outputs.logits)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
            "        [ 4.2141, -3.4158]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.2141, -3.4158]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_3wutsNhdB"
      },
      "source": [
        "## stage3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x34AbmPNiVp",
        "outputId": "80f012b7-707f-45b8-f729-6103a6adacea"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # get from https://huggingface.co/models\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=1) # 为什么-1，实际上1也可以，\n",
        "print(predictions)\n",
        "\n",
        "print(model.config.id2label)\n",
        "\n",
        "\n",
        "print(model.config.id2label[0], float(predictions[0][0])) # 第0句话的0id的概率"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9951e-01, 4.8549e-04]], grad_fn=<SoftmaxBackward>)\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
            "NEGATIVE 0.04019518569111824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpCe8eM7SvKs"
      },
      "source": [
        "# 2.2.5 try it out\n",
        "\n",
        "Choose two (or more) texts of your own and run them through the sentiment-analysis pipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbO92UqmqEJk"
      },
      "source": [
        "pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_UJQd-MSyXx",
        "outputId": "d58ffa0c-fd55-4c7b-ebbd-2dfd637d52a7"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "classifier([\n",
        "    \"Tony Tom Jerry Jim\", # {'label': 'NEGATIVE', 'score': 0.7907747030258179}，置信度不足0.9的时候很可能就很不可靠了\n",
        "    \"Wow, I'm so happy about this tour guide!\",\n",
        "    \"So tired! I can't wait to take a break\"\n",
        "])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.7907747030258179},\n",
              " {'label': 'POSITIVE', 'score': 0.9998595118522644},\n",
              " {'label': 'NEGATIVE', 'score': 0.9996296763420105}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91968x2zqFMt"
      },
      "source": [
        "stage-by-stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpzUSyfKqGtC",
        "outputId": "134e03e3-4307-4d24-b3f2-66c09fd9aeb3"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch  \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "raw_inputs = [\n",
        "    \"Tony Tom Jerry Jim\",\n",
        "    \"Wow, I'm so happy about this tour guide!\",\n",
        "    \"So tired! I can't wait to take a break\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = model(**inputs)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "\n",
        "for i in range(len(raw_inputs)):\n",
        "    print(\"Text:\" + raw_inputs[i], end='\\t')\n",
        "    for key, value in model.config.id2label.items():\n",
        "        print(value + '置信度：' + str(float(predictions[i][key])), end='\\t')\n",
        "    print(\"\")\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:Tony Tom Jerry Jim\tNEGATIVE置信度：0.7907747626304626\tPOSITIVE置信度：0.20922529697418213\t\n",
            "Text:Wow, I'm so happy about this tour guide!\tNEGATIVE置信度：0.0001404868089593947\tPOSITIVE置信度：0.9998594522476196\t\n",
            "Text:So tired! I can't wait to take a break\tNEGATIVE置信度：0.9996296167373657\tPOSITIVE置信度：0.00037032406544312835\t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}