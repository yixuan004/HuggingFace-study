{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-5_Handling Multiple Sequences.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "shdiNohIiEED",
        "4JWZahEuiR7E",
        "nrOqMDhfsCfw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shdiNohIiEED"
      },
      "source": [
        "# pip安装Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPc-ktMBh_gd",
        "outputId": "4851d569-b65a-47bb-fc93-fe1b0678f329"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 20.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 19.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWZahEuiR7E"
      },
      "source": [
        "# 2.5 Overview + 视频学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjVTuXLBjNMh"
      },
      "source": [
        "使用tokenizer对每句话进行逐句的tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wwyYx2tjF6f",
        "outputId": "362f4aa3-3179-4546-b3f9-6df252f992b4"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "print(ids[0])\n",
        "print(ids[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "[1045, 5223, 2023, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2epeSTS1kVWe"
      },
      "source": [
        "pytorch的torch.tensor()方法的输入不接受不相同长度的列表"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "oy0FO9dbkbOW",
        "outputId": "697c1ca8-3975-4b13-a4e2-ed16ec26eb50"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "print(ids)\n",
        "\n",
        "input_ids = torch.tensor(ids)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012], [1045, 5223, 2023, 1012]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-75b9dd84f9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 14 at dim 1 (got 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8j-lgVVk3a1"
      },
      "source": [
        "使用AutoTokenizer实例化后的pad_token_id可以看是用什么进行padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmW0N3Vzk-Az",
        "outputId": "44ddecaf-ebf8-48d1-d484-a7ee722071ee"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "ckpt1 = 'bert-base-uncased'\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(ckpt1)\n",
        "print(tokenizer1.pad_token)\n",
        "print(tokenizer1.pad_token_id)\n",
        "\n",
        "ckpt2 = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(ckpt2)\n",
        "print(tokenizer2.pad_token)\n",
        "print(tokenizer2.pad_token_id)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PAD]\n",
            "0\n",
            "[PAD]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwINLJgIloMv"
      },
      "source": [
        "只进行padding操作，由于缺少首层attentionmask的存在，也会导致不准确的现象产生"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85PQqHs4mB7X",
        "outputId": "ae0aee71-4c6f-410a-f0a2-a929eb6b5bb6"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "\n",
        "tensor_ids0 = torch.tensor([ids[0]]) # 注意torch这里应该变成多个的输入，也就是[[]]二维列表\n",
        "tensor_ids1 = torch.tensor([ids[1]])\n",
        "\n",
        "print(tensor_ids0)\n",
        "print(tensor_ids1)\n",
        "\n",
        "tensor_idsall = torch.tensor(\n",
        "    [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012],\n",
        "     [1045, 5223, 2023, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        ")\n",
        "print(tensor_idsall)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "print(model(tensor_ids0).logits)\n",
        "print(model(tensor_ids1).logits)\n",
        "print(model(tensor_idsall).logits) # 可以看到这里对于ids1的得分计算实际上是错误的\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "tensor([[1045, 5223, 2023, 1012]])\n",
            "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012],\n",
            "        [ 1045,  5223,  2023,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n",
            "tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>)\n",
            "tensor([[-2.7276,  2.8789],\n",
            "        [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9AGJ-QLn2qb"
      },
      "source": [
        "加入attentionmask后，可以得到相同的"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9u0AmsaoPui",
        "outputId": "cbe6fe6c-3dcf-4d91-8fc9-baf0427d48a6"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "\n",
        "tensor_ids0 = torch.tensor([ids[0]]) # 注意torch这里应该变成多个的输入，也就是[[]]二维列表\n",
        "tensor_ids1 = torch.tensor([ids[1]])\n",
        "\n",
        "print(tensor_ids0)\n",
        "print(tensor_ids1)\n",
        "\n",
        "tensor_idsall = torch.tensor(\n",
        "    [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012],\n",
        "     [1045, 5223, 2023, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        ")\n",
        "print(tensor_idsall)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "print(\"gt ids0的 logits\", model(tensor_ids0).logits)\n",
        "print(\"gt ids1的 logits\", model(tensor_ids1).logits)\n",
        "print(\"在model中不携带attention_mask参数：\", model(tensor_idsall).logits) # 可以看到这里对于ids1的得分计算实际上是错误的\n",
        "\n",
        "attention_mask = torch.tensor(\n",
        "    [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "     [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        ")\n",
        "print(\"在model中带上attention_mask参数后：\", model(tensor_idsall, attention_mask=attention_mask).logits)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "tensor([[1045, 5223, 2023, 1012]])\n",
            "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012],\n",
            "        [ 1045,  5223,  2023,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "gt ids0的 logits tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n",
            "gt ids1的 logits tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>)\n",
            "在model中不携带attention_mask参数： tensor([[-2.7276,  2.8789],\n",
            "        [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>)\n",
            "在model中带上attention_mask参数后： tensor([[-2.7276,  2.8789],\n",
            "        [ 3.9497, -3.1357]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h2uQSkepDPR"
      },
      "source": [
        "不用拆分的方式，相对整体的完成这一过程，只要在tokenizer中加入padding=True参数即可"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5TWDQ1pKz_",
        "outputId": "4147f43c-9f1a-4752-ddc3-e5c5fa965488"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\"\n",
        "]\n",
        "\n",
        "input_ids = tokenizer(sentences, padding=True) # 注意这里是一个整体的过程，直接tokenizer()\n",
        "print(input_ids)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrOqMDhfsCfw"
      },
      "source": [
        "# 2.5.1 Models expect a batch of inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0C4KwBbsy41"
      },
      "source": [
        "input的输入应该是一个列表套列表，[[ ... ]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuefKKUYs47S",
        "outputId": "3e9fd073-67ba-4190-87e7-f142dd2a25b2"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# input_ids = torch.tensor(ids) # 会报错 IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
        "input_ids = torch.tensor([ids]) # 改成这样列表套列表就行了\n",
        "\n",
        "model(input_ids)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits',\n",
              "                           tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMvPTZVdvjEW"
      },
      "source": [
        "当我们也加入一个新的维度后，打印inputs_ids和logits，都能正常输出了："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C8KXPolvjgJ",
        "outputId": "6f7f0d14-3e8d-4c03-a2ac-d70465909be2"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification # 简单情感分类的例子\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\" # \n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"input_ids: \", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"logits: \", output.logits)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_ids:  tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "logits:  tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}