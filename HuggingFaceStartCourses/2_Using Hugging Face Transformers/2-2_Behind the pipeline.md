# 2.2 Behind the pipeline

æ³¨ï¼šè¿™æ˜¯ç¬¬ä¸€ä¸ªéœ€è¦åŒºåˆ†ä½¿ç”¨PyTorchè¿˜æ˜¯TensorFlowçš„ç« èŠ‚ï¼ŒæŒ‰ç…§ç›®å‰çš„ç§¯ç´¯æ¥çœ‹è‚¯å®šæ˜¯æ ¹æ®PyTorchæ¥äº†
ä»£ç è§ï¼š
> HuggingFaceStartCourses/2_Using Hugging Face Transformers/2_2_Behind_the_pipeline.ipynb


## 2.2.1 [è§†é¢‘å­¦ä¹ ï¼šWhat happens inside the pipeline function? (PyTorch)](https://youtu.be/1pedAIvTWXk)

![](./MarkdownPictures/2021-08-05-14-27-33.png)

pipelineè¿™ä¸ªç®¡é“å®é™…ä¸Šç”±3ä¸ªé˜¶æ®µç»„æˆã€‚ç¬¬1ä¸ªé˜¶æ®µæ˜¯Tokenizerï¼Œä¹Ÿå°±æ˜¯å°†æºè¯­è¨€è½¬åŒ–ä¸ºä¸€ç§Input IDsï¼›ç¬¬2ä¸ªé˜¶æ®µæ˜¯Modelï¼Œä¹Ÿå°±æ˜¯å°†è¾“å…¥çš„IDsè½¬åŒ–ä¸ºä¸€ç§è¾“å‡ºLogitsï¼›ç¬¬3ä¸ªé˜¶æ®µæ˜¯Post-processingï¼Œå°†é¢„æµ‹å¾—åˆ°çš„æ ‡ç­¾è½¬åŒ–ä¸ºpredictionsçš„label
![](./MarkdownPictures/2021-08-05-14-27-57.png)

### stage1
![](./MarkdownPictures/2021-08-05-14-37-31.png)

å¯¹äºä¸‹è¾¹è¿™ä¸ªå›¾çš„è§£é‡Šæ˜¯ï¼Œé¦–å…ˆæºæ–‡æ¡£æŒ‰ç…§ä¸€äº›æ–¹å¼è¢«åŒ–ä¸ºtokensï¼ˆè‡ªï¼šè¿™ä¸ªè¿‡ç¨‹å¯èƒ½å­˜åœ¨å­—ç¬¦çº§åˆ«ï¼Œå•è¯çº§åˆ«ç­‰å¤šä¸ªçº§åˆ«çš„ã€‚ã€‚ï¼‰ï¼Œä¹‹åè¿™äº›tokensä¼šè¢«åŠ ä¸Šä¸€äº›ä»»åŠ¡æŒ‡å®šï¼Œæˆ–è€…ç”¨æ¥è®­ç»ƒçš„tokensä½œä¸ºä¸€ç§æ‹¼æ¥ï¼Œä¾‹å¦‚å¸¸è§çš„[CLS]ï¼Œ[SEP]ï¼Œæœ€åè¿™äº›special tokensç»è¿‡ä¸€ä¸ªmappingåï¼ˆvocab.txtï¼‰ï¼Œè½¬åŒ–ä¸ºä¸€ä¸ªIDsï¼ˆindexçš„å‘é‡ï¼‰
![](./MarkdownPictures/2021-08-05-14-28-46.png)

ä¸‹å›¾è¿™é‡Œä»£ç åŒ–çš„å±•ç¤ºäº†è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹ï¼Œä»transformersåŠ è½½çš„AutoTokenizerå¯ä»¥éšæ„åŠ è½½checkpointï¼ˆAutoTokenizer.from_pretrainedä½†å¤§æ¦‚ä¹Ÿå¿…é¡»æ˜¯è¿™ä¸ªå¹³å°çš„ï¼Ÿè¿™ä¸ªä¹Ÿåœ¨modelsé‡Œå»æ‰¾å—ï¼Œè¿˜æ˜¯å»ç±»ä¼¼BERTä¸€ç±»çš„æ‰¾ï¼‰ã€‚è¿™é‡Œçš„raw_inputsè¢«è®¾ç½®æˆäº†ä¸€ä¸ªåˆ—è¡¨ï¼Œæœ‰ä¸¤å¥é•¿æ®µä¸ä¸€æ ·çš„è¯ã€‚
```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english" # get from https://huggingface.co/models
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much"
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")

print(inputs)

>>> {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
    2607,  2026,  2878,  2166,  1012,   102],
    [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,     0,
    0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}

```
æ³¨æ„ç©ºæ ¼æ˜¯ä¸ç®—å­—ç¬¦çš„ï¼ï¼è€Œåœ¨è¿™å¥è¯ä¸­ï¼Œâ€œI'veâ€ä¼šè¢«æ‹†åˆ†æˆ[I, ', ve]ã€‚â€œHuggingFaceâ€ä¸€è¯åˆ™è¢«æ‹†åˆ†æˆäº†['hugging', '##face']ï¼Œåœ¨ä¹‹åï¼Œå‰è¾¹åŠ ä¸Šä¸€ä¸ª[CLS]ï¼Œåè¾¹åŠ ä¸Šä¸€ä¸ª[SEP]ï¼Œé•¿åº¦å°±ä¼šå˜æˆ16äº†ã€‚paddingç½®ä¸ºTrueçš„æ—¶å€™ï¼ŒæŒ‰ç…§æœ€é•¿çš„æ¥å¡«å……ï¼Ÿ<font color='red'>truncationå‚æ•°è¿˜éœ€è¦è¿›ä¸€æ­¥ç†è§£</font>ï¼Œpaddingåçš„attention maskä¼šè¢«ææˆ0ï¼Œä»£è¡¨è®¡ç®—æ—¶å¿½ç•¥æ‰ï¼Œåªæ˜¯ä¸ºäº†ææˆç›¸åŒé•¿åº¦
vocat.txtè§ï¼šhttps://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/blob/main/vocab.txt
æ³¨æ„vocat.txtæ˜¯ä»1è¡Œå¼€å§‹æ•°ï¼Œè€Œidsæ˜¯ä»0å¼€å§‹æ•°ï¼Œæ‰€ä»¥è§’æ ‡ä¼šå·®ä¸Šä¸€ä¸ª
![](./MarkdownPictures/2021-08-05-14-29-25.png)


### stage2
æ­¥éª¤2æ˜¯æŠŠè¾“å…¥çš„IDsè½¬åŒ–ä¸ºä¸€ç§æ¦‚ç‡è¡¨ç¤º
```python
from transformers import AutoTokenizer, AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english" # get from https://huggingface.co/models
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much"
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")


model = AutoModel.from_pretrained(checkpoint)
outputs = model(**inputs)

print(outputs.last_hidden_state.shape)
>>> torch.Size([2, 16, 768])ï¼Œåˆ†åˆ«ä»£è¡¨batch_sizeï¼Œåºåˆ—é•¿åº¦ï¼ˆè¾“å…¥åˆ—è¡¨ä¸­æœ€é•¿çš„ï¼ŒåŠ ä¸ŠCLSå’ŒSEPï¼‰ï¼Œè¿˜æœ‰hidden sizeï¼ˆæ¨¡å‹å®šçš„ï¼‰
```
![](./MarkdownPictures/2021-08-05-14-38-48.png)
![](./MarkdownPictures/2021-08-05-14-31-43.png)

æ¯ä¸ªAutoModelForXxxçš„ç±»åŠ è½½ä¸€ä¸ªé€‚åº”äºæŒ‡å®šä»»åŠ¡çš„æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english" # get from https://huggingface.co/models
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much"
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")


model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs)
>>> SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],
    [ 4.2141, -3.4158]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)

print(outputs.logits)
>>> tensor([[-1.5607,  1.6123],
    [ 4.2141, -3.4158]], grad_fn=<AddmmBackward>)
```
![](./MarkdownPictures/2021-08-05-14-32-01.png)


### stage3: 
pipelineçš„æœ€åä¸€ä¸ªæ­¥éª¤æ˜¯å°†logitsè½¬åŒ–ä¸ºpredictionsï¼ˆæœ‰ä¸€ç§å‘æ ‡ç­¾è½¬åŒ–çš„æ„Ÿè§‰ï¼‰
![](./MarkdownPictures/2021-08-05-14-32-45.png)

é¦–å…ˆéœ€è¦æŠŠtensoré€šè¿‡softmaxçš„æ–¹å¼è½¬åŒ–ä¸ºä¸€ç§æ¦‚ç‡è¡¨ç¤º<font color='red'>è¿™é‡Œçš„dimä¸ºä»€ä¹ˆæ˜¯-1éœ€è¦ç†è§£ä¸‹</font>ï¼Œ
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english" # get from https://huggingface.co/models
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much"
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")


model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) # å®é™…ä¸Šç”¨1ä¹Ÿå¯ä»¥ï¼Œå› ä¸ºdim0æ˜¯å¥å­Aï¼Œå¥å­B...ï¼Œè€Œdim1å°±æ˜¯æ¯ä¸ªå¥å­å„ä¸ªlabelçš„å€¼äº†
print(predictions)
>>> tensor([[4.0195e-02, 9.5980e-01],
    [9.9951e-01, 4.8549e-04]], grad_fn=<SoftmaxBackward>)
```
![](./MarkdownPictures/2021-08-05-14-33-20.png)

æœ€åè¿˜éœ€è¦å‘labelè½¬åŒ–ä¸€ä¸‹ï¼Œè¿™ç§åœ°æ–¹çš„è¯modelçš„configä¼šå­˜äº†id2label

```python
print(model.config.id2label[0], float(predictions[0][0])) # ç¬¬0å¥è¯çš„0idçš„æ¦‚ç‡
>>> NEGATIVE 0.04019518569111824
```
![](./MarkdownPictures/2021-08-05-14-33-31.png)

## 2.2.2 Preprocessing with a tokenizer
ä½¿ç”¨è¯ç¬¦åŒ–å™¨tokenizerè¿›è¡Œé¢„å¤„ç†

ä¸å…¶ä»–ç¥ç»ç½‘ç»œä¸€æ ·ï¼ŒTransformeræ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬pipelineçš„ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è¾“å…¥è½¬åŒ–ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨tokenizerï¼Œè¯¥tokenizerå°†è´Ÿè´£ï¼š
* å°†è¾“å…¥æ‹†åˆ†ä¸ºæˆä¸ºtokençš„å•è¯ã€å­å•è¯æˆ–ç¬¦å·ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰
* å°†æ¯ä¸ªä»¤ç‰Œtokenæ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°
* æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„å…¶ä»–è¾“å…¥ï¼ˆä¾‹å¦‚CLSï¼ŒSEPç­‰ï¼‰

æ‰€æœ‰è¿™äº›é¢„å¤„ç†éƒ½éœ€è¦ä»¥ä¸æ¨¡å‹é¢„è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ–¹å¼å®Œæˆï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆéœ€è¦ä»[æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ä¸‹è½½è¿™äº›ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨**AutoTokenizer**ç±»åŠå…¶**from_pretrained**æ–¹æ³•ã€‚ä½¿ç”¨æˆ‘ä»¬æ¨¡å‹çš„æ£€æŸ¥ç‚¹åç§°ï¼Œå®ƒå°†è‡ªåŠ¨è·å–ä¸æ¨¡å‹çš„tokenizerç›¸å…³è”çš„æ•°æ®å¹¶å°†å…¶ç¼“å­˜ï¼ˆå› æ­¤ï¼Œå®ƒä»…åœ¨æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œä¸‹é¢çš„ä»£ç æ—¶ä¸‹è½½ï¼‰ã€‚

ç”±äºæƒ…ç»ªåˆ†æpipelineçš„é»˜è®¤checkpointæ˜¯distilbert-base-uncased-finetened-sst-2-englishï¼ˆ[model card](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬è¿è¡Œä»¥ä¸‹æ“ä½œï¼š

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†tokenizerï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥å°†æˆ‘ä»¬çš„å¥å­ä¼ é€’ç»™å®ƒï¼Œç„¶åæˆ‘ä»¬å°±ä¼šå¾—åˆ°ä¸€æœ¬å­—å…¸ï¼Œå®ƒå¯ä»¥æä¾›ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼å‰©ä¸‹è¦åšçš„å”¯ä¸€ä¸€ä»¶äº‹å°±æ˜¯å°†è¾“å…¥IDåˆ—è¡¨(IDs)è½¬æ¢ä¸ºå¼ é‡(tensors)ã€‚

å¯ä»¥ç”¨Hugging Face Transformerï¼Œè€Œä¸å¿…æ‹…å¿ƒå“ªä¸ªMLæ¡†æ¶ï¼ˆè¿™é‡ŒæŒ‡çš„æ˜¯PyTorchï¼ŒTensorFlowè¿™äº›çš„ï¼‰è¢«ç”¨ä½œåç«¯ã€‚ç„¶è€Œï¼Œtransformersçš„æ¨¡å‹åªæ¥å—tensorä½œä¸ºè¾“å…¥ã€‚å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¬è¯´å¼ é‡ï¼Œå¯ä»¥æŠŠå®ƒä»¬æƒ³è±¡æˆNumPyæ•°ç»„ï¼Œå¯ä»¥æ˜¯æ ‡é‡0Dï¼Œå‘é‡1Dï¼ŒçŸ©é˜µ2Dæˆ–å…·æœ‰æ›´å¤šç»´åº¦ã€‚å®ƒå®é™…ä¸Šæ˜¯å¼ é‡ï¼Œå…¶ä»–MLæ¡†æ¶çš„å¼ é‡è¡Œä¸ºç±»ä¼¼ï¼Œé€šå¸¸ä¸NumPyæ•°ç»„ä¸€æ ·æ˜“äºå®ä¾‹åŒ–

è¦æŒ‡å®šè¦è¿”å›çš„å¼ é‡ç±»å‹ï¼ˆPyTorchã€TensorFlowæˆ–plain NumPyï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨return_tensorså‚æ•°ï¼š
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
æš‚æ—¶ä¸ç”¨å…³æ³¨paddingå’Œtruncationå‚æ•°çš„è®¾ç½®ï¼Œç¨åä¼šå¯¹è¿™äº›è¿›è¡Œè§£é‡Šã€‚è¿™é‡Œè¦è®°ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ï¼Œå¯ä»¥ä¼ é€’ä¸€ä¸ªå¥å­æˆ–ä¸€ç»„å¥å­ï¼Œè¿˜å¯ä»¥æŒ‡å®šè¦è¿”å›çš„å¼ é‡ç±»å‹ï¼ˆå¦‚æœæ²¡æœ‰ä¼ é€’return_tensorsï¼Œå°†å¾—åˆ°ä¸€ç»„åˆ—è¡¨ï¼‰

ä¸€ä¸‹æ˜¯PyTorchå¼ é‡çš„ç»“æœï¼š
```python
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

è¾“å‡ºæœ¬èº«æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œ**input_ids** å’Œ **attention_mask**ã€‚input_idsåŒ…å«ä¸¤è¡Œæ•´æ•°ï¼ˆæ¯ä¸ªå¥å­ä¸€è¡Œï¼‰ï¼Œå®ƒä»¬æ˜¯æ¯ä¸ªå¥å­ä¸­æ ‡è®°çš„å«å”¯ä¸€æ ‡è¯†ç¬¦ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« çš„åç»­å†…å®¹è§£é‡Šä»€ä¹ˆæ˜¯**attention_mask**

## 2.2.3 Going through the model

æˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨tokenizerä¸€æ ·ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ğŸ¤—Transformersæä¾›äº†ä¸€ä¸ªAutoModelç±»ï¼Œè¯¥ç±»è¿˜å…·æœ‰from_pretrainedæ–¹æ³•ï¼š
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
åœ¨è¿™ä¸ªä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä¸‹è½½äº†ä¹‹å‰åœ¨ç®¡é“ä¸­ä½¿ç”¨çš„ç›¸åŒæ£€æŸ¥ç‚¹ï¼ˆå®ƒå®é™…ä¸Šåº”è¯¥å·²ç»è¢«ç¼“å­˜ï¼‰ï¼Œå¹¶ç”¨å®ƒå®ä¾‹åŒ–äº†ä¸€ä¸ªæ¨¡å‹ã€‚

è¿™ä¸ªæ¶æ„åªåŒ…å«åŸºæœ¬transformeræ¨¡å—ï¼šç»™å®šä¸€äº›è¾“å…¥ï¼Œå®ƒè¾“å‡ºæˆ‘ä»¬ç§°ä¹‹ä¸ºéšè—çŠ¶æ€hidden_stateçš„ä¸œè¥¿ï¼Œä¹Ÿç§°ä¸ºç‰¹æ€§featuresã€‚å¯¹äºæ¯ä¸ªæ¨¡å‹è¾“å…¥ï¼Œæˆ‘ä»¬å°†æ£€ç´¢è¡¨ç¤ºTransformeræ¨¡å‹å¯¹è¯¥è¾“å…¥çš„ä¸Šä¸‹æ–‡ç†è§£çš„é«˜ç»´å‘é‡retrieve a high-dimensional vectorã€‚

è™½ç„¶è¿™äº›hidden_stateæœ¬èº«å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯æ¨¡å‹å¦ä¸€éƒ¨åˆ†ï¼ˆç§°ä¸ºå¤´éƒ¨ï¼‰çš„è¾“å…¥ã€‚åœ¨ç¬¬1ç« ä¸­ï¼Œä¸åŒçš„ä»»åŠ¡å¯ä»¥ç”¨ç›¸åŒçš„ä½“ç³»ç»“æ„æ‰§è¡Œï¼Œä½†æ˜¯æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªä¸ä¹‹ç›¸å…³è”çš„ä¸åŒå¤´éƒ¨ã€‚ <font color='red'>-> è¿™ä¸ªheadè¿˜æ˜¯æ²¡æ€ä¹ˆå¬è¯´è¿‡ï¼Œä¸æ˜¯å¾ˆå®Œå…¨æ˜ç™½è¿™é‡Œçš„æ„æ€</font>

### 2.2.3.1 A high-dimensional vector?

Transformersçš„å‘é‡è¾“å‡ºé€šå¸¸è¾ƒå¤§ï¼Œé€šå¸¸å…·æœ‰3ä¸ªç»´åº¦ï¼ˆthree dimensionsï¼‰ï¼š
* Batch sizeï¼šä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°ï¼Œåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º2
* Sequence lengthï¼šåºåˆ—ï¼ˆä¸€å¥è¯çš„ï¼‰æ•°å€¼è¡¨ç¤ºçš„é•¿åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º16ï¼‰
* Hidden sizeï¼šæ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´æ•°ï¼ˆä¸ªäººæ„Ÿè§‰ä¸€èˆ¬ç”±æ¨¡å‹æŒ‡å®šï¼Œæ¯”å¦‚bertçš„768ï¼‰

ç”±äºæœ€åçš„ä¸€ä¸ªå€¼ï¼Œå®ƒè¢«ç§°ä¸ºâ€œé«˜ç»´â€ã€‚hidden_stateçš„å¤§å°å¯èƒ½éå¸¸å¤§ï¼ˆä»¥BERTæ¥è¯´ï¼Œbaseçš„æ¨¡å‹å¯èƒ½æœ‰768ï¼Œè€Œlargeçš„æ¨¡å‹å¯èƒ½æœ‰3072æˆ–æ›´å¤§ï¼‰

å¦‚æœæˆ‘ä»¬å°†é¢„å¤„ç†çš„è¾“å…¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
>>> torch.Size([2, 16, 768])
```
è¯·æ³¨æ„ğŸ¤—Transformersæ¨¡å‹çš„è¡Œä¸ºç±»ä¼¼äº**namedtuple**sæˆ–Dicitionaryã€‚æ‚¨å¯ä»¥é€šè¿‡outputs.last_hidden_stateæ¥è®¿é—®ï¼Œæˆ–è€…ä½¿ç”¨ **outputs["last_hidden_state"]** è¿›è¡Œè®¿é—®ã€‚ç”šè‡³å¦‚æœéå¸¸æ¸…æ¥šçš„è¯ï¼Œç”šè‡³å¯ä»¥ä½¿ç”¨ **output[0]** 

### 2.2.3.2 Model heads: Making sense out of numbers
æ¨¡å‹çš„headsï¼ˆå¯ä»¥è¯´æ˜¯æ¨¡å‹çš„è¾“å‡ºå¤´ï¼Œæœ‰multi headé‚£ä¸ªæ„æ€ï¼‰ï¼Œä½¿å¾—æ•°å­—å…·æœ‰æ„ä¹‰

æ¨¡å‹å¤´å°†éšè—çŠ¶æ€çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°ä¸åŒçš„ç»´åº¦ã€‚å®ƒä»¬é€šå¸¸ç”±ä¸€ä¸ªæˆ–å‡ ä¸ªçº¿æ€§å±‚ç»„æˆï¼š
![](./MarkdownPictures/2021-08-05-16-38-20.png)

<font color='red'>å¯¹è¿™å¼ å›¾è‡ªå·±æè¿°ä¸€ä¸‹ï¼šä¸€ä¸ªè¾“å…¥ç»è¿‡transformeræ¶æ„åä¼šå¾—åˆ°ä¸€ä¸ªhidden_stateçš„è¾“å‡ºï¼Œè¿™ä¸ªè¾“å‡ºè¢«è¾“å…¥åˆ°é’ˆå¯¹ä¸åŒä»»åŠ¡çš„headä¸­ï¼Œå¾—åˆ°å¯¹ä¸åŒä»»åŠ¡çš„model output</font>

Transformerçš„è¾“å‡ºç›´æ¥å‘é€åˆ°model headè¿›è¡Œå¤„ç†ã€‚

åœ¨æ­¤å›¾ä¸­ï¼Œæ¨¡å‹ç”±embeddingå±‚å’Œåç»­å±‚è¡¨ç¤ºã€‚embeddingå±‚å°†æ¯ä¸ªtokenizedåçš„inputIDè½¬åŒ–ä¸ºä¸€ç»„ä¸tokenå…³è”çš„å‘é‡ã€‚åç»­å±‚ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ“çºµè¿™äº›å‘é‡ï¼Œä»¥ç”Ÿæˆå¥å­æœ€ç»ˆè¡¨ç¤ºã€‚

åœ¨Hugging Face Transformerä¸­ï¼Œæœ‰å¾ˆå¤šä¸åŒçš„architectureï¼Œæ¯ä¸€ä¸ªéƒ½æ˜¯å›´ç»•ç€å®Œæˆä¸€é¡¹ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªéè¯¦å°½çš„åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼šAutoModelForSequenceClassificationï¼‰

* \*Model(retrieve the hidden states)
* \*ForCausalLM
* \*ForMaskedLM
* \*ForMultipleChoice
* \*ForQuestionAnswering
* \*ForSequenceClassification
* \*ForTokenClassification

å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„æ¨¡å‹ï¼ˆèƒ½å¤Ÿå°†å¥å­åˆ†ç±»ä¸ºè‚¯å®šæˆ–å¦å®šï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸ä¼šä½¿ç”¨**AutoModel**ç±»ï¼Œè€Œæ˜¯**AutoModelForSequenceClassification**ï¼š
```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```

ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬è§‚å¯Ÿè¾“å…¥çš„å½¢çŠ¶ï¼Œç»´åº¦å°†ä½å¾—å¤šï¼šæ¨¡å‹å¤´å°†æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºåŒ…å«ä¸¤ä¸ªå€¼çš„å‘é‡ï¼ˆæ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªï¼‰ï¼š
```python
print(outputs.logits.shape)
>>> torch.Size([2, 2])
```
å› ä¸ºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªå¥å­å’Œä¸¤ä¸ªæ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„ç»“æœæ˜¯2 x 2çš„å½¢çŠ¶ã€‚

## 2.2.4 Postprocessing the output

æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„è¾“å‡ºå€¼æœ¬èº«ä¸ä¸€å®šæœ‰æ„ä¹‰ï¼Œtake a lookï¼š

```python
print(outputs.logits)
>>> tensor([[-1.5607,  1.6123],
    [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹ç¬¬ä¸€å¥ä¸º[-1.5607,1.6123]ï¼Œç¬¬äºŒå¥ä¸º[4.1692ï¼Œ-3.3464]ã€‚è¿™äº›ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯logitsï¼Œå³æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹ã€æœªè§„èŒƒåŒ–çš„åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡[SoftMax](https://en.wikipedia.org/wiki/Softmax_function)å±‚ã€‚ï¼ˆæ‰€æœ‰Transformeræ¨¡å‹è¾“å‡ºlogitsï¼Œç”¨äºåŸ¹è®­çš„æŸè€—å‡½æ•°é€šå¸¸å°†æœ€åçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚SoftMaxï¼‰ä¸å®é™…æŸè€—å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆï¼‰

![](./MarkdownPictures/2021-08-05-17-18-52.png)
æ³¨ï¼šåœ¨dim 1ä¸Šåšsoftmaxå°±æ˜¯å¯¹æ¯å¥è¯åˆ†åˆ«åšsoftmax

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
>>> tensor([[4.0195e-02, 9.5980e-01],
    [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹é¢„æµ‹ç¬¬ä¸€å¥ä¸º[0.0402,0.9598]ï¼Œç¬¬äºŒå¥ä¸º[0.9995,0.0005]ã€‚è¿™äº›æ˜¯å¯è¯†åˆ«çš„æ¦‚ç‡åˆ†æ•°ã€‚

ä¸ºäº†è·å¾—å¯¹åº”äºæ¯ä¸ªä½ç½®çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹é…ç½®çš„**id2label**å±æ€§ï¼ˆä¸‹ä¸€èŠ‚å°†å¯¹æ­¤è¿›è¡Œè¯¦ç»†ä»‹ç»ï¼‰ï¼š
```python
model.config.id2label
>>> {0: 'NEGATIVE', 1: 'POSITIVE'}
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè¯¥æ¨¡å‹é¢„æµ‹äº†ä»¥ä¸‹å‡ ç‚¹ï¼š
* First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598
* Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005

æˆ‘ä»¬å·²ç»æˆåŠŸåœ°å¤åˆ¶äº†ç®¡é“çš„ä¸‰ä¸ªæ­¥éª¤ï¼šä½¿ç”¨æ ‡è®°åŒ–å™¨ï¼ˆtokenizerï¼‰è¿›è¡Œé¢„å¤„ç†ã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥ä»¥åŠåå¤„ç†ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬èŠ±ä¸€äº›æ—¶é—´æ·±å…¥äº†è§£è¿™äº›æ­¥éª¤ä¸­çš„æ¯ä¸€æ­¥ã€‚

## 2.2.5 Try it Out!

é€‰æ‹©ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰ä½ è‡ªå·±çš„æ–‡æœ¬ï¼Œå¹¶é€šè¿‡æƒ…ç»ªåˆ†æç®¡é“è¿è¡Œå®ƒä»¬ã€‚ç„¶åè‡ªå·±å¤åˆ¶åœ¨è¿™é‡Œçœ‹åˆ°çš„æ­¥éª¤ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦è·å¾—ç›¸åŒçš„ç»“æœï¼

æ–‡æœ¬é€‰æ‹©
> Hello world, this is curious
Wow, I'm so happy about this tour guide!
So tired! I can't wait to take a break

pipelineï¼ŒæŒ‡å®šä»»åŠ¡å’Œæ¨¡å‹å°±å¯ä»¥
```python
from transformers import pipeline

classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
classifier([
    "Tony Tom Jerry Jim", # {'label': 'NEGATIVE', 'score': 0.7907747030258179}ï¼Œç½®ä¿¡åº¦ä¸è¶³0.9çš„æ—¶å€™å¾ˆå¯èƒ½å°±å¾ˆä¸å¯é äº†
    "Wow, I'm so happy about this tour guide!",
    "So tired! I can't wait to take a break"
])

>>> [{'label': 'NEGATIVE', 'score': 0.7907747030258179},
    {'label': 'POSITIVE', 'score': 0.9998595118522644},
    {'label': 'NEGATIVE', 'score': 0.9996296763420105}]
```

stage-by-stageï¼ŒæŒ‡å®šcheckpointï¼ŒåŠ è½½æ¨¡å‹å’Œtokenizerï¼Œè¾“å…¥è¾“å‡ºå¾—åˆ°logitsï¼Œä½¿ç”¨softmaxåˆ†æ•°åŒ–ï¼Œæ ¼å¼åŒ–è¾“å‡º
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch  

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

raw_inputs = [
    "Tony Tom Jerry Jim",
    "Wow, I'm so happy about this tour guide!",
    "So tired! I can't wait to take a break"
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')
outputs = model(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=1)

for i in range(len(raw_inputs)):
    print("Text:" + raw_inputs[i], end='\t')
    for key, value in model.config.id2label.items():
        print(value + 'ç½®ä¿¡åº¦ï¼š' + str(float(predictions[i][key])), end='\t')
    print("")

>>> Text:Tony Tom Jerry Jim	NEGATIVEç½®ä¿¡åº¦ï¼š0.7907747626304626	POSITIVEç½®ä¿¡åº¦ï¼š0.20922529697418213	
    Text:Wow, I'm so happy about this tour guide!	NEGATIVEç½®ä¿¡åº¦ï¼š0.0001404868089593947	POSITIVEç½®ä¿¡åº¦ï¼š0.9998594522476196	
    Text:So tired! I can't wait to take a break	NEGATIVEç½®ä¿¡åº¦ï¼š0.9996296167373657	POSITIVEç½®ä¿¡åº¦ï¼š0.00037032406544312835	
```