## 1. Transformer models

### 1.1 Introduction

#### 1.1.1 Welcom
æœ¬è¯¾ç¨‹å°†æ•™å­¦å¦‚ä½•ä½¿ç”¨[Hugging FaceğŸ¤—](https://huggingface.co/)ç”Ÿæ€ç³»ç»Ÿä¸­çš„åº“è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing, NLPï¼‰ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
* [ğŸ¤—Transformers](https://github.com/huggingface/transformers)
* [ğŸ¤—Datasets](https://github.com/huggingface/datasets)
* [ğŸ¤—Tokenizers](https://github.com/huggingface/tokenizers)
* [ğŸ¤—Accelerate](https://github.com/huggingface/accelerate)
* [ğŸ¤—Hugging Face Hub](https://huggingface.co/models)

### 1.1.2 What to expect?
æœ¬è¯¾ç¨‹çš„ç®€è¦æ¦‚è¿°ï¼ˆæˆªæ­¢2021.8å¯èƒ½åªå‘å¸ƒäº†Introductionï¼‰ï¼š

![](./MarkdownPictures/2021-08-04-09-50-13.png)

Introductionéƒ¨åˆ†åŒ…æ‹¬ï¼šå¯¹Transformer modelsçš„ä»‹ç»ã€ä½¿ç”¨Hugging Face Transformersã€åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œfine-tuneã€åˆ†äº«æ¨¡å‹å’Œè¯ç¬¦åŒ–å™¨ï¼ˆtokenizersï¼‰

Diving inéƒ¨åˆ†åŒ…æ‹¬ï¼šThe Hugging Face Datasets libraryã€The Hugging Face Tokenizers libraryã€ä¸»è¦çš„NLPä»»åŠ¡ã€å¦‚ä½•å¯»æ±‚å¸®åŠ©

Advancedéƒ¨åˆ†åŒ…æ‹¬ï¼šä¸“ç”¨ä½“ç³»ç»“æ„ã€åŠ é€Ÿè®­ç»ƒã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€contributing to Hugging Face

ç¬¬1ç« è‡³ç¬¬4ç« ä»‹ç»äº†Hugging Face Transformers libraryçš„ä¸»è¦ç†è®ºã€‚æœ¬éƒ¨åˆ†è¯¾ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†ç†Ÿæ‚‰Transformeræ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå¹¶å°†ä¹‹é“å¦‚ä½•ä½¿ç”¨Hugging Face Hubä¸­çš„æ¨¡å‹ï¼Œåœ¨æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥åŠåœ¨Hubä¸Šå…±äº«ç»“æœã€‚

ç¬¬5ç« è‡³ç¬¬8ç« åœ¨æ·±å…¥åˆ°ç»å…¸NLPé—®é¢˜å‰ï¼Œæ•™æˆHugging Face Datasetså’ŒHugging Face Tokenizersã€‚åœ¨æœ¬éƒ¨åˆ†ç»“æŸæ—¶ï¼Œå°†èƒ½å¤Ÿè‡ªå·±è§£å†³å¸¸è§çš„NLPé—®é¢˜ã€‚

ç¬¬9ç« è‡³ç¬¬12ç« æ›´æ·±å…¥ï¼Œå±•ç¤ºäº†ä¸“é—¨çš„ä½“ç³»ç»“æ„ï¼ˆå†…å­˜æ•ˆç‡ã€é•¿åºåˆ—ç­‰ï¼‰ï¼Œå¹¶æ•™æ‚¨å¦‚ä½•ä¸ºæ›´å¥‡ç‰¹çš„ç”¨ä¾‹ç¼–å†™è‡ªå®šä¹‰å¯¹è±¡ã€‚åœ¨æœ¬éƒ¨åˆ†ç»“æŸæ—¶ï¼Œæ‚¨å°†å‡†å¤‡å¥½è§£å†³å¤æ‚çš„NLPé—®é¢˜ï¼Œå¹¶ä¸ºè§£å†³è¿™äº›é—®é¢˜contributeåˆ°Hugging Face Transformersä¸Šã€‚

è¯¥è¯¾ç¨‹ï¼š
* éœ€è¦å¯¹Pythonç¨‹åºè®¾è®¡è¯­è¨€æœ‰ä¸€å®šäº†è§£ï¼›
* æœ€å¥½åœ¨ä»‹ç»æ€§çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ä¹‹åè¿›è¡Œï¼Œä¾‹å¦‚é’ˆå¯¹ç¼–ç äººå‘˜çš„å®ç”¨æ·±åº¦å­¦ä¹ æˆ–ç”±deeplearning.aiå¼€å‘çš„è¯¾ç¨‹
* ä¸æœŸæœ›æœ‰PyTorchæˆ–TensorFlowæ–¹é¢çš„çŸ¥è¯†ï¼Œå°½ç®¡ç†Ÿæ‚‰å…¶ä¸­ä»»ä½•ä¸€ç§éƒ½ä¼šæœ‰æ‰€å¸®åŠ©

åœ¨æœ¬ç« èŠ‚ä¸­ï¼Œå°†å­¦ä¹ åˆ°ï¼š

* å¦‚ä½•ä½¿ç”¨pipeline functionæ¥è§£å†³ç±»ä¼¼äºæ–‡æœ¬ç”Ÿæˆï¼ˆtext generationï¼‰å’Œåˆ†ç±»ï¼ˆclassificationï¼‰çš„ä»»åŠ¡ï¼›
* About the Transformer architecture
* å¦‚ä½•åŒºåˆ†ç¼–ç å™¨ã€è§£ç å™¨ï¼Œå’Œç¼–ç å™¨-è§£ç å™¨ä½“ç³»ç»“æ„å’Œç”¨ä¾‹
